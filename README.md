# llm-inference-gateway
Production-oriented LLM inference backend built with FastAPI. Demonstrates stateless API design, OpenAI integration, structured logging, latency measurement, testing, CI, and Docker deployment.
